{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, Dropout, concatenate\n",
    "from keras.layers import Concatenate,Conv2D,BatchNormalization,MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BikeEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        p.connect(p.GUI)\n",
    "        p.setRealTimeSimulation(1)\n",
    "        p.resetDebugVisualizerCamera(cameraDistance=10, cameraYaw=0, cameraPitch=-40, cameraTargetPosition=[0.55,-0.35,0.2])\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self.observation_space = spaces.Box(np.array([-1000]*10), np.array([1000]*10))\n",
    "        self.timestep = 1./240.\n",
    "        \n",
    "    def step(self, action):\n",
    "\n",
    "        if (action == 0):\n",
    "            self.speed = self.speed + 1\n",
    "        if (action == 1):\n",
    "            self.speed = self.speed - 1 \n",
    "        if (action == 2):\n",
    "            self.speed = self.speed  \n",
    "        if (action == 3):\n",
    "            self.steer = self.steer - 1 \n",
    "        if (action == 4):\n",
    "            self.steer = self.steer + 1\n",
    "        if (action == 5):\n",
    "            self.steer = self.steer \n",
    "            \n",
    "              \n",
    "        self.applyAction([self.speed,self.steer])\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        state = p.getLinkState(self.pid,0)[0]\n",
    "        \n",
    "        reward = np.sign(state[0] - self.origin[0])*1\n",
    "        if state[2] <= 0.5 or  state[2] >= 2 or abs(self.speed)>2 or abs(self.steer)>4:\n",
    "            #reward = -100\n",
    "            done = True\n",
    "            \n",
    "        else :\n",
    "            #reward = math.sqrt((self.origin[0]-state[0])**2+(self.origin[1]-state[1])**2)\n",
    "            #reward = state[0] - self.origin[0]\n",
    "            #reward = 1\n",
    "            done = False\n",
    "        self.origin = state \n",
    "        \n",
    "        velocity = p.getBaseVelocity(self.pid)\n",
    "        observation = list(self.getObservation()) + list(velocity[0])+list(velocity[1])\n",
    "        \n",
    "        info = {'x':'','y':'','z':''}\n",
    "        #print(\"Step: \",self.stp)\n",
    "        #xx = time.time()\n",
    "        #print(\"Time: \",xx-self.tttt)\n",
    "        #self.tttt = xx\n",
    "        #print(\"Action: \",action)\n",
    "        #print(\"Reward: \",reward)\n",
    "        #self.stp +=1\n",
    "        return observation, reward, done, info\n",
    "            \n",
    "    def applyAction(self, motorCommands):\n",
    "        targetVelocity = motorCommands[0] * self.speedMultiplier\n",
    "        #print(\"targetVelocity\")\n",
    "        #print(targetVelocity)\n",
    "        steeringAngle = motorCommands[1] * self.steeringMultiplier\n",
    "        #print(\"steeringAngle\")\n",
    "        #print(steeringAngle)\n",
    "\n",
    "\n",
    "        for motor in self.motorizedwheels:\n",
    "            p.setJointMotorControl2(self.pid,\n",
    "                                    motor,\n",
    "                                    p.VELOCITY_CONTROL,\n",
    "                                    targetVelocity=targetVelocity,\n",
    "                                    force=self.maxForce)\n",
    "        for steer in self.steeringLinks:\n",
    "            p.setJointMotorControl2(self.pid,\n",
    "                                    steer,\n",
    "                                    p.POSITION_CONTROL,\n",
    "                                    targetPosition=steeringAngle)\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        #print(\"===========Reset=====\")\n",
    "        self.stp=0\n",
    "        self.tttt= time.time()\n",
    "        p.resetSimulation()\n",
    "\n",
    "        urdfRootPath = pybullet_data.getDataPath()\n",
    "        planeUid = p.loadURDF(os.path.join(urdfRootPath,\"plane.urdf\"), basePosition=[0,0,0])\n",
    "        self.pid = p.loadURDF(os.path.join(urdfRootPath, \"bicycle/bike.urdf\"),basePosition=[0,0,1])\n",
    "        self.origin = p.getLinkState(self.pid,0)[0]\n",
    "        p.setGravity(0,0,-10)\n",
    "        for wheel in range(p.getNumJoints(self.pid)):\n",
    "            p.setJointMotorControl2(self.pid,\n",
    "                                    wheel,\n",
    "                                    p.VELOCITY_CONTROL,\n",
    "                                    targetVelocity=0,\n",
    "                                    force=0)\n",
    "\n",
    "        self.steeringLinks = [0]\n",
    "        self.maxForce = 20\n",
    "        self.nMotors = 2\n",
    "        self.motorizedwheels = [1, 2]\n",
    "        self.speedMultiplier = 10.\n",
    "        self.steeringMultiplier = 0.5\n",
    "        \n",
    "        self.speed = 0 \n",
    "        self.steer = 0\n",
    "\n",
    "        velocity = p.getBaseVelocity(self.pid)\n",
    "        observation = list(self.getObservation()) + list(velocity[0])+list(velocity[1])\n",
    "        p.configureDebugVisualizer(p.COV_ENABLE_RENDERING,1)\n",
    "        return observation\n",
    "        \n",
    "    def getObservationDimension(self):\n",
    "        return len(self.getObservation())\n",
    "    \n",
    "    def getObservation(self):\n",
    "        observation = []\n",
    "        pos, orn = p.getBasePositionAndOrientation(self.pid)\n",
    "\n",
    "        #observation.extend(list(pos))\n",
    "        observation.extend(list(orn))\n",
    "        return observation\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        view_matrix = p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0.7,0,0.05],\n",
    "                                                            distance=.7,\n",
    "                                                            yaw=90,\n",
    "                                                            pitch=-70,\n",
    "                                                            roll=0,\n",
    "                                                            upAxisIndex=2)\n",
    "        proj_matrix = p.computeProjectionMatrixFOV(fov=60,\n",
    "                                                     aspect=float(960) /720,\n",
    "                                                     nearVal=0.1,\n",
    "                                                     farVal=100.0)\n",
    "        (_, _, px, _, _) = p.getCameraImage(width=960,\n",
    "                                              height=720,\n",
    "                                              viewMatrix=view_matrix,\n",
    "                                              projectionMatrix=proj_matrix,\n",
    "                                              renderer=p.ER_BULLET_HARDWARE_OPENGL)\n",
    "\n",
    "        rgb_array = np.array(px, dtype=np.uint8)\n",
    "        rgb_array = np.reshape(rgb_array, (720,960, 4))\n",
    "\n",
    "        rgb_array = rgb_array[:, :, :3]\n",
    "        return rgb_array\n",
    "\n",
    "    def close(self):\n",
    "        p.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iDQNAgent:\n",
    "    def __init__(self, state_space , action_space, episodes=500 , memory_size = 50000):\n",
    "        self.action_space = action_space\n",
    "        self.memory = []\n",
    "        self.memory_size = memory_size\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = self.epsilon_min / self.epsilon\n",
    "        self.epsilon_decay = self.epsilon_decay ** (1. / float(episodes))\n",
    "        \n",
    "        n_inputs = state_space.shape[0]\n",
    "        n_outputs = action_space.n\n",
    "        \n",
    "        self.q_model = self.build_model(n_inputs , n_outputs)\n",
    "        self.q_model.compile(loss='mse' , optimizer=Adam())\n",
    "        \n",
    "        self.target_q_model = self.build_model(n_inputs , n_outputs)\n",
    "        \n",
    "        self.update_weights()\n",
    "        self.replay_counter = 0\n",
    "        \n",
    "        ####################################################\n",
    "    def build_model(self, n_inputs , n_outputs):\n",
    "        inputs = Input(shape = (n_inputs,) , name='state')\n",
    "        x = Dense(256 , activation='relu')(inputs)\n",
    "        x = Dense(256 , activation='relu')(x)\n",
    "        x = Dense(256 , activation='relu')(x)\n",
    "        x = Dense(n_outputs , activation='linear' , name = 'action')(x)\n",
    "        model = Model(inputs , x)\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "        #####################################################\n",
    "    def act(self , state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.action_space.sample()\n",
    "        q_values = self.q_model.predict(state)\n",
    "        action = np.argmax(q_values[0])\n",
    "        return action\n",
    "    \n",
    "        ######################################################\n",
    "    def remember(self, state, action, reward , next_state , done):\n",
    "        item = (state , action , reward , next_state ,done)\n",
    "        if len(self.memory) > self.memory_size :\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append(item)\n",
    "        \n",
    "        #######################################################\n",
    "    def get_target_q_value(self, next_state, reward , double):\n",
    "        if double :\n",
    "            action = np.argmax(self.q_model.predict(next_state[0]))\n",
    "            q_value = self.target_q_model.predict(next_state)[0][action]\n",
    "        else :\n",
    "            q_value = np.amax(self.target_q_model.predict(next_state)[0])\n",
    "        \n",
    "        q_value *= self.gamma\n",
    "        q_value += reward\n",
    "        return q_value\n",
    "    \n",
    "        ########################################################\n",
    "    def replay(self , batch_size):\n",
    "        sars_batch = random.sample(self.memory , batch_size)\n",
    "       \n",
    "        state_batch , q_value_batch = [] , []\n",
    "        for state, action, reward, next_state, done in sars_batch:\n",
    "            q_values = self.q_model.predict(state)\n",
    "            \n",
    "            q_value = self.get_target_q_value(next_state, reward, False)\n",
    "            # ??????????\n",
    "            q_values[0][action] = reward if done else q_value\n",
    "            \n",
    "            state_batch.append(state[0])\n",
    "            q_value_batch.append(q_values[0])\n",
    "            \n",
    "        hist = self.q_model.fit(np.array(state_batch) , \n",
    "                             np.array(q_value_batch),\n",
    "                             batch_size = batch_size,\n",
    "                            epochs = 1,\n",
    "                            verbose = 0)\n",
    "\n",
    "        self.update_epsilon()\n",
    "            \n",
    "        if self.replay_counter % 10 == 0:\n",
    "            self.update_weights()\n",
    "                \n",
    "        self.replay_counter += 1\n",
    "        \n",
    "        return hist.history['loss'][0]\n",
    "       \n",
    "       ######################################################\n",
    "\n",
    "       ######################################################\n",
    "    def update_epsilon(self):\n",
    "            if self.epsilon > self.epsilon_min :\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "        ########################################################\n",
    "    def update_weights(self):\n",
    "            self.target_q_model.set_weights(self.q_model.get_weights())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = BikeEnv()\n",
    "np.random.seed(123)\n",
    "env.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_count = 5000\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "state (InputLayer)           (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2816      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "action (Dense)               (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 135,942\n",
      "Trainable params: 135,942\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "state (InputLayer)           (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               2816      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "action (Dense)               (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 135,942\n",
      "Trainable params: 135,942\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\SER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = iDQNAgent(env.observation_space ,env.action_space, episode_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 reward -8.0 loss 0\n",
      "episode 1 reward 3.0 loss 0\n",
      "episode 2 reward -4.0 loss 0\n",
      "episode 3 reward -6.0 loss 0\n",
      "episode 4 reward -7.0 loss 0\n",
      "episode 5 reward -1.0 loss 0\n",
      "episode 6 reward -1.0 loss 0\n",
      "episode 7 reward -5.0 loss 0\n",
      "episode 8 reward -9.0 loss 0\n",
      "episode 9 reward -8.0 loss 0\n",
      "episode 10 reward -9.0 loss 0\n",
      "episode 11 reward -1.0 loss 0\n",
      "episode 12 reward -5.0 loss 0\n",
      "episode 13 reward -5.0 loss 0\n",
      "episode 14 reward 3.0 loss 0\n",
      "episode 15 reward -4.0 loss 0\n",
      "episode 16 reward -5.0 loss 0\n",
      "episode 17 reward -3.0 loss 0\n",
      "episode 18 reward 5.0 loss 0\n",
      "episode 19 reward 4.0 loss 0\n",
      "episode 20 reward -3.0 loss 0\n",
      "episode 21 reward -6.0 loss 0\n",
      "episode 22 reward -7.0 loss 0\n",
      "episode 23 reward -8.0 loss 0\n",
      "episode 24 reward 3.0 loss 0\n",
      "episode 25 reward -5.0 loss 0\n",
      "episode 26 reward -4.0 loss 0\n",
      "episode 27 reward 3.0 loss 0\n",
      "episode 28 reward -8.0 loss 0\n",
      "episode 29 reward -5.0 loss 0\n",
      "episode 30 reward -7.0 loss 0\n",
      "episode 31 reward 4.0 loss 0\n",
      "episode 32 reward -5.0 loss 0\n",
      "episode 33 reward -1.0 loss 0\n",
      "episode 34 reward -8.0 loss 0\n",
      "episode 35 reward 5.0 loss 0\n",
      "episode 36 reward 2.0 loss 0\n",
      "episode 37 reward -2.0 loss 0\n",
      "episode 38 reward 1.0 loss 0\n",
      "episode 39 reward 4.0 loss 0\n",
      "episode 40 reward -9.0 loss 0.1569558084011078\n",
      "episode 41 reward 3.0 loss 0.14980007708072662\n",
      "episode 42 reward 6.0 loss 0.1367274522781372\n",
      "episode 43 reward 8.0 loss 0.1267642378807068\n",
      "episode 44 reward -1.0 loss 0.11688155680894852\n",
      "episode 45 reward 5.0 loss 0.1092132180929184\n",
      "episode 46 reward -7.0 loss 0.10047900676727295\n",
      "episode 47 reward 6.0 loss 0.09329432994127274\n",
      "episode 48 reward -8.0 loss 0.08701477944850922\n",
      "episode 49 reward -3.0 loss 0.08057218044996262\n",
      "episode 50 reward -3.0 loss 0.07510079443454742\n",
      "episode 51 reward 7.0 loss 0.13677120208740234\n",
      "episode 52 reward -3.0 loss 0.11899073421955109\n",
      "episode 53 reward 3.0 loss 0.1070815697312355\n",
      "episode 54 reward 7.0 loss 0.09750799834728241\n",
      "episode 55 reward 5.0 loss 0.09336894005537033\n",
      "episode 56 reward 3.0 loss 0.08722493797540665\n",
      "episode 57 reward -7.0 loss 0.08307156711816788\n",
      "episode 58 reward 7.0 loss 0.0800730437040329\n",
      "episode 59 reward -6.0 loss 0.07890816032886505\n",
      "episode 60 reward 6.0 loss 0.0657333955168724\n",
      "episode 61 reward -8.0 loss 0.17932772636413574\n",
      "episode 62 reward -2.0 loss 0.16437093913555145\n",
      "episode 63 reward -6.0 loss 0.15322761237621307\n",
      "episode 64 reward 6.0 loss 0.126776322722435\n",
      "episode 65 reward -7.0 loss 0.12339898943901062\n",
      "episode 66 reward 4.0 loss 0.11371315270662308\n",
      "episode 67 reward -3.0 loss 0.11403387784957886\n",
      "episode 68 reward -8.0 loss 0.10659536719322205\n",
      "episode 69 reward 5.0 loss 0.10959428548812866\n",
      "episode 70 reward -12.0 loss 0.08760501444339752\n",
      "episode 71 reward -6.0 loss 0.2229737639427185\n",
      "episode 72 reward -5.0 loss 0.191173255443573\n",
      "episode 73 reward -8.0 loss 0.18817028403282166\n",
      "episode 74 reward 5.0 loss 0.14003250002861023\n",
      "episode 75 reward 3.0 loss 0.1413295865058899\n",
      "episode 76 reward 3.0 loss 0.1309661865234375\n",
      "episode 77 reward -2.0 loss 0.1335117518901825\n",
      "episode 78 reward 1.0 loss 0.11776989698410034\n",
      "episode 79 reward 1.0 loss 0.10570405423641205\n",
      "episode 80 reward -7.0 loss 0.09827756881713867\n",
      "episode 81 reward 8.0 loss 0.19243814051151276\n",
      "episode 82 reward 5.0 loss 0.1683964729309082\n",
      "episode 83 reward 3.0 loss 0.15080437064170837\n",
      "episode 84 reward 3.0 loss 0.12638363242149353\n",
      "episode 85 reward 0.0 loss 0.11392506957054138\n",
      "episode 86 reward 9.0 loss 0.11720781028270721\n",
      "episode 87 reward -6.0 loss 0.1379064917564392\n",
      "episode 88 reward -8.0 loss 0.08935636281967163\n",
      "episode 89 reward 6.0 loss 0.11363118886947632\n",
      "episode 90 reward -7.0 loss 0.12391946464776993\n",
      "episode 91 reward -2.0 loss 0.18017730116844177\n",
      "episode 92 reward -7.0 loss 0.16570709645748138\n",
      "episode 93 reward -7.0 loss 0.16541573405265808\n",
      "episode 94 reward -7.0 loss 0.12108400464057922\n",
      "episode 95 reward -7.0 loss 0.12393423914909363\n",
      "episode 96 reward -7.0 loss 0.11434020847082138\n",
      "episode 97 reward -5.0 loss 0.1437644362449646\n",
      "episode 98 reward -5.0 loss 0.10927878320217133\n",
      "episode 99 reward 5.0 loss 0.0986250787973404\n",
      "episode 100 reward 6.0 loss 0.11695147305727005\n",
      "episode 101 reward 3.0 loss 0.11313880980014801\n",
      "episode 102 reward -5.0 loss 0.1264958530664444\n",
      "episode 103 reward -4.0 loss 0.10023930668830872\n",
      "episode 104 reward 2.0 loss 0.09451611340045929\n",
      "episode 105 reward -7.0 loss 0.09857498109340668\n",
      "episode 106 reward -3.0 loss 0.08541566133499146\n",
      "episode 107 reward -2.0 loss 0.08780189603567123\n",
      "episode 108 reward 5.0 loss 0.09007278829813004\n",
      "episode 109 reward 3.0 loss 0.10369497537612915\n",
      "episode 110 reward 1.0 loss 0.0883980393409729\n",
      "episode 111 reward 7.0 loss 0.11866727471351624\n",
      "episode 112 reward 4.0 loss 0.08355538547039032\n",
      "episode 113 reward 2.0 loss 0.0818316638469696\n",
      "episode 114 reward -2.0 loss 0.09703622758388519\n",
      "episode 115 reward -7.0 loss 0.10228388756513596\n",
      "episode 116 reward 3.0 loss 0.09026716649532318\n",
      "episode 117 reward 5.0 loss 0.09886065870523453\n",
      "episode 118 reward 3.0 loss 0.10579696297645569\n",
      "episode 119 reward 1.0 loss 0.09418557584285736\n",
      "episode 120 reward 14.0 loss 0.09384705871343613\n",
      "episode 121 reward -6.0 loss 0.10251227021217346\n",
      "episode 122 reward -2.0 loss 0.09189069271087646\n",
      "episode 123 reward -5.0 loss 0.09737201035022736\n",
      "episode 124 reward 4.0 loss 0.08089695870876312\n",
      "episode 125 reward -8.0 loss 0.10435976088047028\n",
      "episode 126 reward -5.0 loss 0.07041995972394943\n",
      "episode 127 reward 3.0 loss 0.07817982137203217\n",
      "episode 128 reward 6.0 loss 0.08184053003787994\n",
      "episode 129 reward -7.0 loss 0.10170842707157135\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0bd46be3d708>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mnext_state\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-1bb7931d3fc0>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplyAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspeed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLinkState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_rewards = []\n",
    "all_losses =  []\n",
    "loss = 0\n",
    "for episode in range(episode_count):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state , [1,state_size])\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done :\n",
    "        action = agent.act(state)\n",
    "        next_state , reward , done , _ = env.step(action)\n",
    "        next_state = np.reshape(next_state , [1,state_size])\n",
    "        agent.remember(state , action , reward , next_state , done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    \n",
    "    if len(agent.memory) >= batch_size:\n",
    "        loss = agent.replay(batch_size)\n",
    "        all_losses.append(loss)\n",
    "        \n",
    "    print(\"episode {} reward {} loss {}\".format(episode , total_reward, loss))\n",
    "    all_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(episode_count),all_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(all_losses)),all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.q_model.save_weights(\"d:\\\\iDQM-bike\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
